<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Question Answering | Your awesome title</title>
    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- Add MathJax script here -->
    <script type="text/javascript"
            async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
    </script>
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      };
    </script>
  </head>
  <body>
    <header>
      <h1>Your awesome title</h1>
    </header>
    <main>
      <article>
  <header>
    <h1>Question Answering</h1>
    <p>11 Jan 2023</p>
  </header>
  <section>
    <blockquote>
  <p>factoid questions: questions that can be answered with simple facts expressed in short text</p>

  <ul>
    <li>information retrieval based QA</li>
    <li>knowledge-based QA</li>
  </ul>
</blockquote>

<h2 id="information-retrieval">information retrieval</h2>

<blockquote>
  <p>retrieving all manner of media based on user information needs (search engine)
![[Screenshot 2022-11-20 at 14.11.48.png]]</p>
</blockquote>

<ul>
  <li>
    <h3 id="how-a-query-and-a-document-match">how a query and a document match</h3>

    <ul>
      <li>
        <h4 id="term-weight-for-document-words">term weight for document words</h4>

        <ul>
          <li>tf-idf
            <ul>
              <li>term frequency tell us how frequent the word is (more frequent words are likely to be more informative about the document’s contents)
                <ul>
                  <li>we use $log_{10}$ instead of raw count(one word appearing 100 times does not mean it is 100 times more important), and add 1 to the count because we can’t log 0 $tf_{t,d} = log_{10}(count(t,d)+1)$</li>
                </ul>
              </li>
              <li>document frequency of a term is the number of document it occurs in (words that occur only in a few documents are more useful for discriminating the documents)
                <ul>
                  <li>define inverse document frequency $idf_t = log_{10}\frac{N}{df_t}$ the fewer documents a word occurs in, the higher this weight (more discriminating)</li>
                </ul>
              </li>
              <li>tf-idf value $tf-idf(t,d) = tf_{t,d}\cdot idf_t$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>document scoring</p>

        <ul>
          <li>
            <p>cosine similarity for scoring a document</p>

\[score(q,d)=cos(q,d)=\frac{q\cdot d}{|q|\cdot|d|}  = \frac{q}{|q|}\cdot\frac{d}{|d|}\]

            <p>where q is a query vector and d is document vector, spell out equation using td-idf values and dot product as a sum of products:</p>

\[score(q,d) = \sum_{t\in q}\frac{tf\text{-}idf(t,q)}{\sqrt{\sum_{q_i\in q}tf\text{-}idf^2(q_i,q)}}\cdot \frac{tf\text{-}idf(t,d)}{\sqrt{\sum_{d_i\in d}tf\text{-}idf^2(d_i,d)}}\]
          </li>
          <li>
            <p>However, we can simplify the query processing, use the following simple score:</p>

\[score(q,d) = \sum_{t\in q}\frac{tf\text{-}idf(t,d)}{|d|}\]
          </li>
        </ul>
      </li>
      <li>
        <h4 id="bm25-as-another-weight-scheme"><strong>BM25</strong> as another weight scheme</h4>

        <ul>
          <li>k as the knob that adjust the balance between term frequency and IDF</li>
          <li>b controls the importance of document length normalization</li>
        </ul>
      </li>
      <li>
        <h4 id="inverted-index">Inverted Index</h4>

        <ul>
          <li>given a query term, gives a list of candidate documents</li>
          <li>compositions:
            <ul>
              <li>dictionary: a list of terms, each pointing to a positings list for the term</li>
              <li>postings: the list of document IDs associated with each term ![[inverted index.png]]</li>
              <li>indexing based on bigrams works better than unigram</li>
              <li>hashing algorithms are better than the inverted index</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <h3 id="evaluation-of-ir-systems">evaluation of IR systems</h3>

    <ul>
      <li>
        <p><strong>Precision and Recall</strong></p>

\[Precision = \frac{|R|}{|T|} \ \ \ \ \ Recall = \frac{|R|}{|U|}\]

        <p>is relevant documents, T is returned ranked documents, U is all relevant documents in the whole collection</p>

        <ul>
          <li>Precision and recall are not adequate, to capture how well a system does at putting relevant documents higher in the ranking:
            <ul>
              <li>
                <p>interpolated precision: choose the maximum precision value achieved at any level of recall at or aobve the one we are calculating</p>

\[IntPrecision = \mathop{max}\limits_{i&gt;=r}Precision(i)\]
              </li>
              <li>
                <p>compare two systems or approaches by comparing their precision-recall curves</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Mean average precision</strong> (MAP)</p>
        <ul>
          <li>
            <p>gives a single metric that can be used to compare systems</p>
          </li>
          <li>
            <p>note precision only at those points where a relevant item has been encountered</p>
            <ul>
              <li>
                <p>assume $R_r$ is the set of relevant documents at or above r, average precision is:</p>

\[AP = \frac{1}{|R_r|}\sum_{d\in R_r}Precision_r(d)\]
              </li>
              <li>
                <p>$Precision_r(d)$ is the precision measured at the rank where document d was found, for an ensemble of queries, we average over averages:</p>

\[MAP = \frac{1}{|Q|}\sum_{q\in Q}AP(q)\]
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <h4 id="ir-with-dense-vectors">IR with dense vectors</h4>
  </li>
</ul>

<blockquote>
  <p><strong><em>tf-idf and BM25 algorithms only work if there is exact overlap of words between the query and document, it is likely to have vocabulary mismatch problem</em></strong></p>
</blockquote>

<ul>
  <li>modern approaches use encoders like BERT
    <ul>
      <li>more complex versions like using averaging pooling over the BERT outputs or add extra weight matrices after encoding or dot product steps</li>
      <li>for efficiency, modern systems use nearest neighbor vector search algorithms like <strong>Faiss</strong>.</li>
    </ul>
  </li>
  <li>
    <h2 id="ir-based-factoid-question-answering">IR-based Factoid Question Answering</h2>

    <ul>
      <li>also <strong>open domain QA</strong>, answer a user’s questions by finding short text segments from the web or other large collection of documents</li>
      <li><strong>retrieve and read model</strong> ![[retrieve and read model.png]]</li>
      <li>text retrieval</li>
      <li>reading comprehension</li>
    </ul>
  </li>
  <li>
    <h4 id="datasets">Datasets</h4>

    <ul>
      <li>SQuAD</li>
      <li>HotpotQA</li>
      <li>TriviaQA</li>
      <li>Natural Questions</li>
      <li>TyDi QA (non-English)</li>
    </ul>
  </li>
  <li>
    <h4 id="reader">Reader</h4>

    <ul>
      <li>for extractive QA, the answer that reader produces is a span of text in the passage</li>
      <li>standard baseline algorithm is to pass the question and passage to any encoder like BERT</li>
      <li>BERT allows up to 512 tokens, for longer passages, we create multiple pseudo-passage observations</li>
    </ul>
  </li>
  <li>
    <h2 id="entity-linking">Entity Linking</h2>

    <ul>
      <li>
        <p>EL is the task of associating a mention in text with representation of some real-world entity in an ontology</p>
      </li>
      <li>
        <p>EL is done in two stages:</p>

        <ul>
          <li>mention detection</li>
          <li>mention disambiguation</li>
        </ul>
      </li>
      <li>
        <h4 id="linking-based-on-anchor-dictionaries-and-web-graph">Linking based on Anchor Dictionaries and Web Graph</h4>

        <ul>
          <li>TAGME linker [[Fast and accurate annotation of short texts with Wikipedia pages.pdf]]
            <ul>
              <li>anchor dictionary</li>
              <li>linke probabillity</li>
            </ul>
          </li>
          <li>Mention Detection
            <ul>
              <li>query the anchor dictionary for each token sequence</li>
            </ul>
          </li>
          <li>Mention Disambiguation
            <ul>
              <li>
                <p>Spans match anchors for multiple Wikipedia entities/pages</p>
              </li>
              <li>
                <p>prior probability</p>

\[prior(e\rightarrow a) = p(e|a) = \frac{count(a \rightarrow e)}{link(a)}\]

                <p>This gives the link of the highest probability, but it is not always correct.</p>
              </li>
              <li>
                <p>relatedness/coherence</p>

\[rel(A,B)=\frac{log(max(|in(A)|,|in(B)|))-log(|in(A)|\cap |in(B)|)}{log(|W|)-log(min(|in(A)|, |in(B)|))}\]

                <p>W is the collection of all pages</p>
              </li>
              <li>
                <p>vote given by anchor b to the candidate annotation a to X is</p>

\[vote(b,X)=\frac{1}{|\mathcal{E}(b)|}\sum_{Y\in \mathcal{E}(b)}rel(X,Y)p(Y,b)\]
              </li>
              <li>
                <p>Total relatedness score for a to X is</p>

\[relatedness(a\rightarrow X)=\sum_{b\in \mathcal{X}\backslash a}vote(b,X)\]
              </li>
              <li>
                <p>see book for more references.</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <h4 id="neural-graph-based-linking">Neural Graph-based linking</h4>

        <ul>
          <li><strong>biencoders</strong> allows embeddings for all the entities in the knowledge based to be prcomputed and cached [[Scalable Zero-shot Entity Linking with Dense Entity Retrieval.pdf]]</li>
          <li>ELQ linking algorithm [[Efficient One-Pass End-to-End Entity Linking for Questions.pdf]]
            <ul>
              <li>Entity Mention Detection</li>
              <li>Entity Linking</li>
              <li>Training</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <h2 id="knowledge-based-question-answering-1719df">Knowledge-based Question Answering ^1719df</h2>

    <ul>
      <li><strong>Graph-based QA</strong>
        <ul>
          <li>from RDF triple stores: a set of factoids</li>
          <li>RDF: a predicate with two arguements, expressing some simple relation or proposition \(&lt;subject, predicate, object&gt;\)</li>
          <li>datasets
            <ul>
              <li>SimpleQuestions-Freebase</li>
              <li>FreebaseQA</li>
              <li>WEBQUESTIONS</li>
              <li>COMPLEXWEBQUESTIONS</li>
            </ul>
          </li>
          <li>steps:
            <ul>
              <li>entity linking</li>
              <li>mapping from question to canonical relations in knowledge base (triple)</li>
              <li>relation detection and linking
                <ul>
                  <li>compute similarity (dot product) between the encoding of the question text and an encoding for each possible relation</li>
                </ul>
              </li>
              <li>ranking of answers
                <ul>
                  <li>heuristic</li>
                  <li>train a classifier (concatenated entity/relation encodings -&gt; predict a probability)</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>QA by semantic parsing</strong>
        <ul>
          <li>uses a semantic parser to map the question to a structured program to produce an answer</li>
          <li>predicate calculus
            <ul>
              <li>can be converted to SQL</li>
            </ul>
          </li>
          <li>query language
            <ul>
              <li>SQL</li>
              <li>SPARQL</li>
            </ul>
          </li>
          <li>Semantic parsing algorithms
            <ul>
              <li>fully supervised with questions paried with a hand-built logical form [[Logical Representations of Sentence Meaning]]
                <ul>
                  <li>a set of question paired with their correct logical form
                    <ul>
                      <li>GEOQUERY</li>
                      <li>DROP</li>
                      <li>ATIS</li>
                    </ul>
                  </li>
                  <li>take those pairs of training tuples and produce a system that maps from new questions to their logical forms
                    <ul>
                      <li>baseline: simple sequence-to-sequence model</li>
                      <li>[[Computational Semantics and Semantic Parsing]]</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>weakly supervised by questions paired with an answer</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <h2 id="using-language-models-to-do-qa">Using Language Models to do QA</h2>

    <ul>
      <li>query a pretrained language model, answer a question solely from information stored in its  parameters
        <ul>
          <li>T5 langauge model, encoder-decoder architecture [[How Much Knowledge Can You Pack Into the Parameters of a Language Model.pdf]]</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <h2 id="classic-qa-models---watson-deepqa-system">Classic QA Models - Watson DeepQA system</h2>

    <ul>
      <li><strong>Question Processing</strong>
        <ul>
          <li>named entities are extracted</li>
          <li>question focus is the string of words in the question that corefers with the answer</li>
          <li>lexical answer type: a word or words which indicate the smenatic type of the answer</li>
        </ul>
      </li>
      <li><strong>Candidate Answer Generation</strong>
        <ul>
          <li>query structured resources with relations and known enetities</li>
          <li>extract answers from text
            <ul>
              <li>IR stage to get passages</li>
              <li>extract anchor texts and all noun phrases from passages</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Candidate Answering Scoring</strong>
        <ul>
          <li>a classifier that scores whether the candidate answer can be interpreted as a subcalss or instance of the potential answer type</li>
          <li>use time and space relations extracted from structured database</li>
          <li>use text retrieval to retrieve evidence</li>
          <li>output is a set of candidate answers, each with a vector of scoring features</li>
        </ul>
      </li>
      <li><strong>Answer merging and scoring</strong></li>
    </ul>
  </li>
  <li>
    <h2 id="evaluation-of-factoid-answers">Evaluation of Factoid Answers</h2>
  </li>
</ul>

  </section>
</article>

    </main>
    <footer>
      <p>&copy; 2024 Your awesome title</p>
    </footer>
  </body>
</html>
